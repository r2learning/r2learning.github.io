<!DOCTYPE html>
<html lang="en-US">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-124409515-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-124409515-1');
    </script>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Relational Representation Learning | NeurIPS 2018 Workshop</title>
<meta name="generator" content="Jekyll v3.6.2" />
<meta property="og:title" content="Relational Representation Learning" />
<meta name="author" content="Aditya Grover" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="NeurIPS 2018 Workshop" />
<meta property="og:description" content="NeurIPS 2018 Workshop" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="Relational Representation Learning" />
<script type="application/ld+json">
{"name":"Relational Representation Learning","description":"NeurIPS 2018 Workshop","author":{"@type":"Person","name":"Aditya Grover"},"@type":"WebSite","url":"http://localhost:4000/","headline":"Relational Representation Learning","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/assets/css/style.css?v=">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Relational Representation Learning</h1>
      <h2 class="project-tagline">NeurIPS 2018 Workshop</h2>
      <a href="#overview" class="btn">Overview</a>
      <a href="#speakers" class="btn">Speakers</a>
      <a href="#schedule" class="btn">Schedule</a>
      <a href="#organizers" class="btn">Organizers</a>
      <a href="#papers" class="btn">Papers</a>
    </section>

    <section class="main-content">
      <h2 id="-overview"><a name="overview"></a> Overview</h2>

<p><strong>Date and Time:</strong> 8:45 AM - 6:00 PM, December 8, 2018<br />
<strong>Location:</strong> Room 517A, Palais des Congrès de Montréal, Montréal, Canada</p>

<p>Relational reasoning, <em>i.e.</em>, learning and inference with relational data, is key to understanding how objects interact with each other and give rise to complex phenomena in the everyday world. Well-known applications include knowledge base completion and social network analysis. Although many relational datasets are available, integrating them directly into modern machine learning algorithms and systems that rely on continuous, gradient-based optimization and make strong i.i.d. assumptions is challenging. Relational representation learning has the potential to overcome these obstacles: it enables the fusion of recent advancements like deep learning and relational reasoning to learn from high-dimensional data. Success of such methods can facilitate novel applications of relational reasoning in areas such as scene understanding, visual question-answering, understanding chemical and biological processes, program synthesis and analysis, decision-making in multi-agent systems and many others.</p>

<p>How should we rethink classical representation learning theory for relational representations? Classical approaches based on dimensionality reduction techniques such as isoMap and spectral decompositions still serve as strong baselines and are slowly paving the way for modern methods in relational representation learning based on random walks over graphs, message-passing in neural networks, group-invariant deep architectures etc. amongst many others. How can systems be designed and potentially deployed for large scale representation learning? What are promising avenues, beyond traditional applications like knowledge base and social network analysis, that can benefit from relational representation learning?</p>

<p>This workshop aims to bring together researchers from both academia and industry interested in addressing various aspects of representation learning for relational reasoning.</p>

<h2 id="-invited-speakers--panelists"><a name="speakers"></a> Invited Speakers &amp; Panelists</h2>

<p><em>Speakers</em><br />
<a href="https://cims.nyu.edu/~bruna/">Joan Bruna</a>, New York University  <br />
<a href="https://homes.cs.washington.edu/~pedrod/">Pedro Domingos</a>, University of Washington <br />
<a href="https://getoor.soe.ucsc.edu/home">Lise Getoor</a>, University of California, Santa Cruz <br />
<a href="http://contrastiveconvergence.net/~timothylillicrap/index.php">Timothy Lillicrap</a>, Google Deepmind   <br />
<a href="https://www.stat.washington.edu/mmp/">Marina Meila</a>, University of Washington <br />
<a href="https://mnick.github.io/">Maximilian Nickel</a>, Facebook Artificial Intelligence Research</p>

<p><em>Panelists</em><br />
<a href="http://aditya-grover.github.io/">Aditya Grover</a>, Stanford University<br />
<a href="https://williamleif.github.io/">William Hamilton</a>, McGill/Facebook Artificial Intelligence Research<br />
<a href="http://www.jesshamrick.com/">Jessica Hamrick</a>, Google Deepmind<br />
<a href="https://tkipf.github.io/">Thomas Kipf</a>, University of Amsterdam<br />
<a href="https://paroma.github.io/">Paroma Varma</a>, Stanford University<br />
<a href="https://stanford.edu/~marinka/">Marinka Zitnik</a>, Stanford University</p>

<h2 id="-schedule"><a name="schedule"></a> Schedule</h2>

<table>
  <thead>
    <tr>
      <th><strong>Time</strong></th>
      <th><strong>Event</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>8:30-8:45 AM                </td>
      <td>Welcome</td>
    </tr>
    <tr>
      <td>8:45-9:00 AM</td>
      <td><strong>Contributed Talk:</strong> Charlotte Bunne <br /> <em>Learning Generative Models across Incomparable Spaces</em></td>
    </tr>
    <tr>
      <td>9:00-9:30 AM</td>
      <td><strong>Invited Talk:</strong> Marina Meila</td>
    </tr>
    <tr>
      <td>9:30-9:45 AM</td>
      <td><strong>Contributed Talk:</strong> Lingfei Wu <br /> <em>From Node Embedding to Graph Embedding: Scalable Global Graph Kernel via Random Features</em></td>
    </tr>
    <tr>
      <td>9:45-10:15 AM</td>
      <td><strong>Invited Talk:</strong>  Timothy Lillicrap</td>
    </tr>
    <tr>
      <td>10:15-10:30 AM</td>
      <td><strong>Poster Spotlights Talks</strong></td>
    </tr>
    <tr>
      <td>10:30-11:00 AM</td>
      <td>Coffee Break + Poster Session 1</td>
    </tr>
    <tr>
      <td>11:00-11:30 AM</td>
      <td><strong>Invited Talk:</strong>  Joan Bruna  <br /> <em>Community Detection with Non-backtracking Graph Neural Networks</em></td>
    </tr>
    <tr>
      <td>11:30-11:45 AM</td>
      <td><strong>Contributed Talk:</strong> Yunsheng Bai <br /> <em>Convolutional Set Matching for Graph Similarity</em></td>
    </tr>
    <tr>
      <td>11:45-12:15 PM</td>
      <td><strong>Invited Talk:</strong>  Maximilian Nickel</td>
    </tr>
    <tr>
      <td>12:15-2:00 PM</td>
      <td>Lunch</td>
    </tr>
    <tr>
      <td>2:00-2:30 PM</td>
      <td><strong>Invited Talk:</strong> Lise Getoor <br /> <em>The Power of Structure: Exploiting Relationships for Representation Learning</em></td>
    </tr>
    <tr>
      <td>2:30-2:45 PM</td>
      <td><strong>Contributed Talk:</strong> Robert Csordas <br /> <em>Improved Addressing in the Differentiable Neural Computer</em></td>
    </tr>
    <tr>
      <td>2:45-3:00 PM</td>
      <td><strong>Poster Spotlight Talks</strong></td>
    </tr>
    <tr>
      <td>3:00-3:30 PM</td>
      <td>Coffee Break + Poster Session 2</td>
    </tr>
    <tr>
      <td>3:30-4:00 PM</td>
      <td><strong>Invited Talk:</strong>  Pedro Domingos <br /> <em>The Power of Objects and Relations in Deep Reinforcement Learning</em></td>
    </tr>
    <tr>
      <td>4:00-4:45 PM</td>
      <td><strong>Panel</strong> <br /> Moderator: Paroma Varma <br /> Panelists: Aditya Grover, William Hamilton, Jessica Hamrick, Thomas Kipf, Marinka Zitnik</td>
    </tr>
    <tr>
      <td>4:45-5:45 PM</td>
      <td>Poster Session</td>
    </tr>
    <tr>
      <td>5:45-6:00 PM</td>
      <td>Closing Remarks</td>
    </tr>
  </tbody>
</table>

<!-- ## <a name="submission"></a> Call for Papers
### Paper Submission Instructions
Workshop papers should be at most **4 pages of content**, including text and figures. Additional pages containing only bibliographic references can be included without penalty. 

We welcome and encourage position papers on this subject. We are also particularly interested in papers that introduce datasets and competitions to further progress in the field.

### Submission Guidelines
Workshop papers should be at most **4 pages of content**, including text and figures, excluding references. Authors can include an appendix of supplementary material after the references. However, reviewers will not be required to consult any appendices to make their decisions. The main 4-page paper should adequately describe the work and its contributions.

Papers should be anonymized and adhere to the NeurIPS conference format: [https://neurips.cc/Conferences/2018/PaperInformation/StyleFiles](https://neurips.cc/Conferences/2018/PaperInformation/StyleFiles)

**Submission Site:** [https://cmt3.research.microsoft.com/R2L2018](https://cmt3.research.microsoft.com/R2L2018)

### Peer Review and Acceptance Criteria
All submissions will go through a double-blind peer review process. Accepted papers will be chosen based on techincal merit, interest, and novelty. The workshop allows submissions of papers that are under review or have been recently published in a conference or a journal. Authors should state any overlapping published work at the time of submission.

All accepted papers will be included in one of the two poster presentation and lightning talk sessions on the day of the workshop. Some accepted papers will be invited to give contributed oral talks. Final versions of accepted papers will be posted on the workshop website. These are archival but do not constitute a proceedings and can be submitted elsewhere.

All accepted papers will be included in one of the two poster presentation and lightning talk sessions on the day of the workshop. Some accepted papers will be invited to give a contributed oral talks.  -->

<!-- ### Important Dates 
* **Submission Deadline: October 19th, 2018, 23:59 PST**
* Notification of Acceptance: November 2nd, 2018
* Camera-ready Due: November 16th, 2018
* Workshop: December 8, 2018 -->

<h2 id="-organizers"><a name="organizers"></a> Organizers</h2>
<p><a href="http://aditya-grover.github.io/">Aditya Grover</a>, Stanford University<br />
<a href="https://paroma.github.io/">Paroma Varma</a>, Stanford University <br />
<a href="https://stanford.edu/~fredsala/">Fred Sala</a>, Stanford University<br />
<a href="https://web.cs.ucla.edu/~sholtzen/">Steven Holtzen</a>, University of California, Los Angeles<br />
<a href="https://www.cs.purdue.edu/homes/neville/index.html">Jennifer Neville</a>, Purdue University<br />
<a href="https://cs.stanford.edu/~ermon/">Stefano Ermon</a>, Stanford University<br />
<a href="https://cs.stanford.edu/people/chrismre/">Christopher Ré</a>, Stanford University</p>

<p><strong>Contact: <a href="mailto:r2learning@googlegroups.com">r2learning@googlegroups.com</a></strong></p>

<h2 id="-accepted-papers"><a name="papers"></a> Accepted Papers</h2>
<ul>
  <li><a href="assets/papers/hippo-r2l-camera-ready.pdf">On the Complexity of Exploration in Goal-driven Navigation</a>. Maruan Al-Shedivat, Lisa Lee, Ruslan Salakhutdinov, Eric Xing</li>
  <li>Improving Knowledge Graph Embeddings with Inferred Entity Types. Esma Balkir, Masha Naslidnyk, Dave Palfrey, Arpit Mittal</li>
  <li><a href="http://petar-v.com/dgi_nips18_camera.pdf">Deep Graph Infomax</a>. Petar Veličković, Liam Fedus, William Hamilton, Pietro Liò, Yoshua Bengio, Devon Hjelm</li>
  <li><a href="https://arxiv.org/pdf/1811.03830.pdf">Image-Level Attentional Context Modeling Using Nested-Graph Neural Networks</a>. Guillaume Jaume, Behzad Bozorgtabar, Hazim Kemal Ekenel, Jean-Philippe Thiran, Maria Gabrani</li>
  <li><a href="assets/papers/CameraReadySubmission 8.pdf">Compositional Language Understanding with Text-based Relational Reasoning</a>. Koustuv Sinha, Shagun Sodhani, William L Hamilton, Joelle Pineau</li>
  <li><a href="http://www.ar.sanken.osaka-u.ac.jp/pub/yoneda/NIPSWS2018_cr1113.pdf">Learning Graph Representation via Formal Concept Analysis</a>. Yuka Yoneda, Mahito Sugiyama, Takashi Washio</li>
  <li><a href="https://arxiv.org/pdf/1810.09155.pdf">A Simple Baseline Algorithm for Graph Classification</a>. Nathan De Lara, Edouard Pineau</li>
  <li><a href="https://arxiv.org/pdf/1811.05868.pdf">Pitfalls of Graph Neural Network Evaluation</a>. Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, Stephan Günnemann</li>
  <li><a href="https://abdcelikkanat.github.io/projects/TNE/TNE_R2L2018.pdf">TNE: A Latent Model for Representation Learning on Networks</a>. Abdulkadir Celikkanat, Fragkiskos Malliaros</li>
  <li>Using Ternary Rewards to Reason over Knowledge Graphs with Deep Reinforcement Learning. Frederic Godin, Anjishnu Kumar, Arpit Mittal</li>
  <li><a href="https://drive.google.com/open?id=1HtZqg5mFXm3xeM-8Q8O1DlAd-2apGaik">A Case for Object Compositionality in GANs</a>. Sjoerd van Steenkiste, Karol Kurach, Sylvain Gelly</li>
  <li><a href="https://zelda.lids.mit.edu/wp-content/uploads/sites/17/2018/11/nips_workshop.pdf">Learning DPPs by Sampling Inferred Negatives</a>. Zelda Mariet, Mike Gartrell, Suvrit Sra</li>
  <li><a href="http://www.cs.toronto.edu/~rjliao/papers/NIPS_R2L_lanczos_net.pdf">LanczosNet: Multi-Scale Deep Graph Convolutional Networks</a>. Renjie Liao, Zhizhen Zhao, Raquel Urtasun, Richard Zemel</li>
  <li><a href="http://www.berkkapicioglu.com/wp-content/uploads/2018/11/chess2vec_nips_2018_short.pdf">Chess2vec: Learning Vector Representations for Chess</a>. Berk Kapicioglu, Ramiz Iqbal</li>
  <li><a href="http://wushanshan.github.io/files/GraphicalModel_workshop.pdf">Sparse Logistic Regression Learns All Discrete Pairwise Graphical Models</a>. Shanshan Wu, Sujay Sanghavi, Alex Dimakis</li>
  <li><a href="http://petar-v.com/spcls_nips18_camera.pdf">Towards Sparse Hierarchical Graph Classifiers</a>. Catalina Cangea, Petar Veličković, Nikola Jovanović, Thomas Kipf, Pietro Liò</li>
  <li><a href="https://drive.google.com/file/d/1UYsTSnyKjl6MAox9vwGtV77wB_3vMavR/view?usp=sharing">GRevnet: Improving Graph Neural Nets with Reversible Computation</a>. Aviral Kumar, Jimmy Ba, Jamie Kiros, Kevin Swersky</li>
  <li><a href="https://www.mis.mpg.de/preprints/2018/preprint2018_97.pdf">Detecting the Coarse Geometry of Networks</a>. Melanie Weber, Emil Saucan, Jürgen Jost</li>
  <li><a href="https://xiaoranxu.com/files/attflow_short_Xu.pdf">Modeling Attention Flow on Graphs</a>. Xiaoran Xu</li>
  <li><a href="https://www.bunne.ch/paper/Bunne_2018_NeurIPS_R2L.pdf">Learning Generative Models across Incomparable Spaces</a>. Charlotte Bunne, David Alvarez-Melis, Andreas Krause , Stefanie Jegelka</li>
  <li><a href="https://drive.google.com/file/d/1cJabrT7Y_HN2DTuIkJz7kWiAFIlz9OOt/view?usp=sharing">Hierarchical Bipartite Graph Convolution Networks</a>. Marcel Nassar</li>
  <li><a href="https://drive.google.com/open?id=1idZrhvIL8n2rGWyz2x3fLOByrqeSsRz5">Non-local RoI for Cross-Object Perception</a>. Shou-Yao Tseng, Hwann-Tzong Chen, Shao-Heng Tai, Tyng-Luh Liu</li>
  <li><a href="http://stanford.edu/~jugander/papers/neurips18w-withinacross.pdf">Node Attribute Prediction: An Evaluation of Within- versus Across-Network Tasks</a>. Kristen M. Altenburger, Johan Ugander</li>
  <li>Implicit Maximum Likelihood Estimation. Ke Li, Jitendra Malik</li>
  <li><a href="https://arxiv.org/pdf/1806.08672.pdf">Variational learning across domains with triplet information</a>. Rita Kuznetsova</li>
  <li>Fast k-Nearest Neighbour Search via Prioritized DCI. Ke Li, Jitendra Malik</li>
  <li><a href="https://arxiv.org/pdf/1811.07245.pdf">Deep Determinantal Point Processes</a>. Mike Gartrell, Elvis Dohmatob</li>
  <li><a href="http://sami.haija.org/papers/high-order-gc-layer.pdf">Higher-Order Graph Convolutional Layer</a>. Sami A Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Hrayr Harutyunyan</li>
  <li><a href="http://yunshengb.com/wp-content/uploads/2018/11/Convolutional_Set_Matching_for_Graph_Similarity.pdf">Convolutional Set Matching for Graph Similarity</a>. Yunsheng Bai, Hao Ding, Yizhou Sun, Wei Wang</li>
  <li><a href="https://arxiv.org/pdf/1811.04784.pdf">Improving Generalization for Abstract Reasoning Tasks Using Disentangled Feature Representations</a>. Xander Steenbrugge, Tim Verbelen, Bart Dhoedt, Sam Leroux</li>
  <li><a href="assets/papers/RGE_NIPS18_RRL_Workshop.pdf">From Node Embedding to Graph Embedding: Scalable Global Graph Kernel via Random Features</a>. Lingfei Wu, Ian En-Hsu Yen, Kun Xu, Liang Zhao, Yinglong Xia, Michael Witbrock</li>
  <li><a href="http://www.ccs.neu.edu/home/clara/resources/neural-framework-learning.pdf">A Neural Framework for Learning DAG to DAG Translation</a>. M. Clara De Paolis Kaluza, Saeed Amizadeh, Rose Yu</li>
  <li><a href="https://priyeshv.github.io/R2L_SSNMF.pdf">Semi-supervised learning for clusterable graph embeddings with NMF</a>. Priyesh Vijayan, Anasua Mitra, Srinivasan Parthasarathy, Balaraman Ravindran</li>
  <li><a href="http://www.cse.iitd.ac.in/~mausam/papers/nipswork18.pdf">Lifted Inference for Faster Training in end-to-end neural-CRF models</a>. Yatin Nandwani, Ankit Anand, Mausam , Parag Singla</li>
  <li><a href="https://arxiv.org/pdf/1811.07174.pdf">Link Prediction in Dynamic Graphs for Recommendation</a>. Samuel G. Fadel, Ricardo Torres</li>
  <li>Curvature and Representation Learning: Identifying Embedding Spaces for Relational Data. Maximillian Nickel</li>
  <li><a href="https://arxiv.org/pdf/1811.02798.pdf">Multi-Task Graph Autoencoders</a>. Phi Vu Tran</li>
  <li><a href="assets/papers/CameraReadySubmission 2.pdf">Personalized Neural Embeddings for Collaborative Filtering with Text</a>. Guangneng Hu, Yu Zhang</li>
  <li><a href="assets/papers/CameraReadySubmission 3.pdf">Symbolic Relation Networks for Reinforcement Learning</a>. Dhaval D Adjodah, Tim Klinger, Josh Joseph</li>
  <li><a href="assets/papers/CameraReadySubmission 11.pdf">Extending the Capacity of CVAE for Face Sythesis and Modeling</a>. Shengju Qian, Wayne Wu, Yangxiaokang Liu, Beier Zhu, Fumin Shen</li>
  <li><a href="assets/papers/CameraReadySubmission 49.pdf">SARN: Relational Reasoning through Sequential Attention</a>. Jinwon An, Seongwon Lyu, Sungzoon Cho</li>
  <li><a href="https://arxiv.org/pdf/1811.06405.pdf">Pairwise Relational Networks using Local Appearance Features for Face Recognition</a>. Bong-Nam Kang, YongHyun Kim, Daijin Kim</li>
  <li><a href="assets/papers/CameraReadySubmission 35.pdf">Compositional Fairness Constraints for Graph Embeddings</a>. Avishek Bose, William L Hamilton</li>
  <li><a href="http://people.idsia.ch/~csordas/nips2018.pdf">Improved Addressing in the Differentiable Neural Computer</a>. Róbert Csordás, Jürgen Schmidhuber</li>
  <li><a href="https://bigdata1.research.cs.dal.ca/behrouz/publication/nipsw2018/NIPSW2018_EfficientWordSenseDisambiguation.pdf">Efficient Unsupervised Word Sense Induction, Disambiguation and Embedding</a>. Behrouz Haji Soleimani, Habibeh Naderi, Stan Matwin</li>
  <li><a href="assets/papers/CameraReadySubmission 19.pdf">Importance of object selection in Relational Reasoning tasks</a>. Kshitij Dwivedi, Gemma Roig</li>
  <li><a href="http://erikml.com/on_robust_learning_of_ising_models.pdf">On Robust Learning of Ising Models</a>. Erik Lindgren, Vatsal Shah, Yanyao Shen, Alex Dimakis, Adam Klivans</li>
  <li><a href="assets/papers/CameraReadySubmission 53.pdf">Feed-Forward Neural Networks need Inductive Bias to Learn Equality Relations</a>. Tillman Weyde, Radha Manisha Kopparti</li>
  <li><a href="assets/papers/CameraReadySubmission 41.pdf">Tensor Random Projection for Low Memory Dimension Reduction</a>. Yang Guo, Yiming Sun, Madeleine Udell, Joel Tropp</li>
  <li><a href="assets/papers/CameraReadySubmission 42.pdf">Leveraging Representation and Inference through Deep Relational Learning</a>. Maria Leonor Pacheco, Ibrahim Dalal, Dan Goldwasser</li>
  <li><a href="assets/papers/CameraReadySubmission 48.pdf">Learning Embeddings for Approximate Lifted Inference in MLNs</a>. Maminur Islam, Somdeb Sarkhel, Deepak Venugopal</li>
  <li><a href="assets/papers/CameraReadySubmission 39.pdf">Quantum Machine Learning on Knowledge Graphs</a>. Yunpu Ma, Volker Tresp</li>
</ul>

<h2 id="-program-committee"><a name="committee"></a> Program Committee</h2>

<ul>
  <li>Albert Gu, Stanford University</li>
  <li>Alexander Gaunt, Microsoft Research</li>
  <li>Alexander Ratner, Stanford University</li>
  <li>Avner May, Stanford University</li>
  <li>Beliz Gunel, Stanford University</li>
  <li>Bryan He, Stanford University</li>
  <li>Bryan Perozzi, Stonybrook University</li>
  <li>Changping Meng, Purdue University</li>
  <li>Daniel Levy, Stanford University</li>
  <li>Daniel Kang, Stanford University</li>
  <li>Golnoosh Farnadi, UC Santa Cruz</li>
  <li>Guilherme Gomes, Purdue University</li>
  <li>Happy Mittal, IIT Delhi</li>
  <li>Hima Lakkaraju, Harvard University</li>
  <li>Jared Dunnmon, Stanford University</li>
  <li>Jiaming Song, Stanford University</li>
  <li>Jian Zhang, Stanford University</li>
  <li>Jiasen Yang, Purdue University</li>
  <li>Jiaxuan You, Stanford University</li>
  <li>Kristy Choi, Stanford University</li>
  <li>Marinka Zitnik, Stanford University</li>
  <li>Maruan Al-Shedivat, CMU</li>
  <li>Max Lam, Stanford University</li>
  <li>Megan Leszczynski, Stanford University</li>
  <li>Mengyue Hang, Purdue University</li>
  <li>Nikolaos Vasiloglou, RelationalAI</li>
  <li>Oleksandr Polozov, Microsoft Research</li>
  <li>Rex Ying, Stanford University</li>
  <li>Sen Wu, Stanford University</li>
  <li>Tal Friedman, UCLA</li>
  <li>Thomas Kipf, University of Amsterdam</li>
  <li>Tony Ginart, Stanford University</li>
  <li>Tri Dao, Stanford University</li>
  <li>William Hamilton, McGill/Facebook Artificial Intelligence Research</li>
  <li>Yang Song, Stanford University</li>
  <li>Yitao Liang, UCLA</li>
  <li>Yujia Li, DeepMind</li>
  <li>Zhaobin Kuang, Stanford University</li>
</ul>


      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>